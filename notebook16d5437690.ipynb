{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3649,"databundleVersionId":46718,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\ny_train, y_test = tf.keras.utils.to_categorical(y_train, 10), tf.keras.utils.to_categorical(y_test, 10)\n\n# Normalize images\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Load pre-trained MobileNetV2\nbase_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\nbase_model.trainable = False  # Freeze base model\n\n# Modify the model\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutputs = Dense(10, activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=outputs)\n\n# Compile model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Data augmentation\ndatagen = ImageDataGenerator(\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True)\ndatagen.fit(x_train)\n\n# Train model\nhistory = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n                    validation_data=(x_test, y_test),\n                    epochs=10)\n\n# Fine-tune: Unfreeze some layers\nfor layer in base_model.layers[-50:]:\n    layer.trainable = True\n\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train again with fine-tuning\nhistory_fine = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n                         validation_data=(x_test, y_test),\n                         epochs=10)\n\n# Save model\nmodel.save(\"fine_tuned_cnn.h5\")\n\n# Load and test the model\nfrom tensorflow.keras.models import load_model\nmodel = load_model(\"fine_tuned_cnn.h5\")\nloss, accuracy = model.evaluate(x_test, y_test)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-22T22:09:26.811865Z","iopub.execute_input":"2025-02-22T22:09:26.812234Z","iopub.status.idle":"2025-02-22T22:19:34.995260Z","shell.execute_reply.started":"2025-02-22T22:09:26.812206Z","shell.execute_reply":"2025-02-22T22:19:34.994596Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-5-5d04158fc2b9>:19: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 42ms/step - accuracy: 0.2300 - loss: 2.0972 - val_accuracy: 0.3055 - val_loss: 1.9241\nEpoch 2/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - accuracy: 0.2975 - loss: 1.9256 - val_accuracy: 0.3202 - val_loss: 1.8928\nEpoch 3/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.3102 - loss: 1.9013 - val_accuracy: 0.3265 - val_loss: 1.8714\nEpoch 4/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.3125 - loss: 1.8898 - val_accuracy: 0.3303 - val_loss: 1.8615\nEpoch 5/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.3178 - loss: 1.8775 - val_accuracy: 0.3328 - val_loss: 1.8519\nEpoch 6/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.3163 - loss: 1.8783 - val_accuracy: 0.3331 - val_loss: 1.8454\nEpoch 7/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.3250 - loss: 1.8709 - val_accuracy: 0.3305 - val_loss: 1.8406\nEpoch 8/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.3253 - loss: 1.8595 - val_accuracy: 0.3396 - val_loss: 1.8293\nEpoch 9/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - accuracy: 0.3290 - loss: 1.8549 - val_accuracy: 0.3370 - val_loss: 1.8286\nEpoch 10/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - accuracy: 0.3247 - loss: 1.8619 - val_accuracy: 0.3372 - val_loss: 1.8265\nEpoch 1/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 46ms/step - accuracy: 0.1981 - loss: 4.8621 - val_accuracy: 0.1580 - val_loss: 2.9812\nEpoch 2/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.3280 - loss: 1.8499 - val_accuracy: 0.2775 - val_loss: 1.9793\nEpoch 3/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.4117 - loss: 1.6434 - val_accuracy: 0.4330 - val_loss: 1.5897\nEpoch 4/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.4575 - loss: 1.5204 - val_accuracy: 0.5102 - val_loss: 1.4168\nEpoch 5/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.4921 - loss: 1.4490 - val_accuracy: 0.5559 - val_loss: 1.2770\nEpoch 6/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.5232 - loss: 1.3710 - val_accuracy: 0.5817 - val_loss: 1.1744\nEpoch 7/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.5517 - loss: 1.3004 - val_accuracy: 0.5918 - val_loss: 1.1486\nEpoch 8/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.5742 - loss: 1.2400 - val_accuracy: 0.6147 - val_loss: 1.1299\nEpoch 9/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.5972 - loss: 1.1859 - val_accuracy: 0.6320 - val_loss: 1.1058\nEpoch 10/10\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.6126 - loss: 1.1369 - val_accuracy: 0.6233 - val_loss: 1.1049\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6200 - loss: 1.1048\nTest Accuracy: 62.33%\n","output_type":"stream"}],"execution_count":5}]}